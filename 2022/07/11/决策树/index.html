<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#0078d4"><meta name="generator" content="Hexo 6.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#0078d4">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"hetan697.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.12.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"flat"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"utterances","storage":true,"lazyload":true,"nav":null,"activeClass":"utterances"},"stickytabs":true,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInDown"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="使用C++和python实现ID3算法的决策树模型，另有使用Pythonsklearn.tree中的DecisionTreeClassifier调包与可视化实例。">
<meta property="og:type" content="article">
<meta property="og:title" content="决策树">
<meta property="og:url" content="http://hetan697.github.io/2022/07/11/%E5%86%B3%E7%AD%96%E6%A0%91/index.html">
<meta property="og:site_name" content="Hetan的博客">
<meta property="og:description" content="使用C++和python实现ID3算法的决策树模型，另有使用Pythonsklearn.tree中的DecisionTreeClassifier调包与可视化实例。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://habrastorage.org/webt/z0/yp/wm/z0ypwmvjko4r2mymcohy7ovevwe.png">
<meta property="og:image" content="https://habrastorage.org/files/f9f/3b5/133/f9f3b5133bae460ba96ab7e546155b1d.png">
<meta property="article:published_time" content="2022-07-11T12:42:51.000Z">
<meta property="article:modified_time" content="2022-07-14T14:43:36.114Z">
<meta property="article:author" content="Hetan">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://habrastorage.org/webt/z0/yp/wm/z0ypwmvjko4r2mymcohy7ovevwe.png">


<link rel="canonical" href="http://hetan697.github.io/2022/07/11/%E5%86%B3%E7%AD%96%E6%A0%91/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://hetan697.github.io/2022/07/11/%E5%86%B3%E7%AD%96%E6%A0%91/","path":"2022/07/11/决策树/","title":"决策树"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>决策树 | Hetan的博客</title>
  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Hetan的博客</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E5%8E%9F%E7%90%86"><span class="nav-number">1.</span> <span class="nav-text">决策树原理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%86%B5"><span class="nav-number">1.1.</span> <span class="nav-text">熵</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A"><span class="nav-number">1.2.</span> <span class="nav-text">信息增益</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E4%B8%AD%E7%9A%84%E8%AF%84%E5%88%A4%E6%A0%87%E5%87%86"><span class="nav-number">1.3.</span> <span class="nav-text">分类问题中的评判标准</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="nav-number">2.</span> <span class="nav-text">优缺点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E4%BE%8B"><span class="nav-number">3.</span> <span class="nav-text">实例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="nav-number">3.1.</span> <span class="nav-text">分类问题</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E7%94%9F%E6%88%90"><span class="nav-number">3.1.1.</span> <span class="nav-text">数据生成</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="nav-number">3.1.2.</span> <span class="nav-text">数据可视化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E6%90%AD%E5%BB%BA%E4%B8%8E%E8%AE%AD%E7%BB%83"><span class="nav-number">3.1.3.</span> <span class="nav-text">模型搭建与训练</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A0%91%E6%A8%A1%E5%9E%8B%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="nav-number">3.1.4.</span> <span class="nav-text">树模型可视化</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98"><span class="nav-number">3.2.</span> <span class="nav-text">回归问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E5%86%B3%E7%AD%96%E6%A0%91%E8%BF%9B%E8%A1%8C-MNIST-%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB"><span class="nav-number">3.3.</span> <span class="nav-text">使用决策树进行 MNIST 手写数字识别</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE"><span class="nav-number">3.3.1.</span> <span class="nav-text">加载数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MNIST-%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="nav-number">3.3.2.</span> <span class="nav-text">MNIST 可视化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%92%E5%88%86"><span class="nav-number">3.3.3.</span> <span class="nav-text">数据集划分</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%BB%BA%E7%AB%8B%E4%B8%8E%E8%AE%AD%E7%BB%83"><span class="nav-number">3.3.4.</span> <span class="nav-text">模型建立与训练</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B%E4%B8%8E%E8%AF%84%E5%AE%9A"><span class="nav-number">3.3.5.</span> <span class="nav-text">模型预测与评定</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B0%83%E5%8F%82"><span class="nav-number">3.3.6.</span> <span class="nav-text">调参</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="nav-number">3.3.7.</span> <span class="nav-text">交叉验证</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B0%83%E5%BA%93"><span class="nav-number">4.</span> <span class="nav-text">调库</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#sklearn-tree-DecisionTreeClassifier"><span class="nav-number">4.1.</span> <span class="nav-text">sklearn.tree.DecisionTreeClassifier</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Classifier%E5%8F%AF%E9%80%89%E5%8F%82%E6%95%B0"><span class="nav-number">4.1.1.</span> <span class="nav-text">Classifier可选参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Classifier%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B"><span class="nav-number">4.1.2.</span> <span class="nav-text">Classifier使用示例</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sklearn-tree-DecisionTreeRegressor"><span class="nav-number">4.2.</span> <span class="nav-text">sklearn.tree.DecisionTreeRegressor</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Regressor%E5%8F%AF%E9%80%89%E5%8F%82%E6%95%B0"><span class="nav-number">4.2.1.</span> <span class="nav-text">Regressor可选参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Regressor%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B"><span class="nav-number">4.2.2.</span> <span class="nav-text">Regressor使用示例</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0"><span class="nav-number">5.</span> <span class="nav-text">实现</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ID3-%E5%AE%9E%E7%8E%B0"><span class="nav-number">5.1.</span> <span class="nav-text">ID3 实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#C4-5"><span class="nav-number">5.2.</span> <span class="nav-text">C4.5</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B5%84%E6%BA%90"><span class="nav-number">6.</span> <span class="nav-text">资源</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83"><span class="nav-number">7.</span> <span class="nav-text">参考</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Hetan</p>
  <div class="site-description" itemprop="description">普普通通准研究生</div>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/hetan697" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hetan697" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://hetan697.github.io/2022/07/11/%E5%86%B3%E7%AD%96%E6%A0%91/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Hetan">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hetan的博客">
      <meta itemprop="description" content="普普通通准研究生">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="决策树 | Hetan的博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          决策树
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-07-11 20:42:51" itemprop="dateCreated datePublished" datetime="2022-07-11T20:42:51+08:00">2022-07-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-07-14 22:43:36" itemprop="dateModified" datetime="2022-07-14T22:43:36+08:00">2022-07-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>使用<code>C++</code>和<code>python</code>实现<code>ID3算法</code>的决策树模型，另有使用Python<code>sklearn.tree</code>中的<code>DecisionTreeClassifier</code>调包与可视化实例。</p>
<span id="more"></span>

<h2 id="决策树原理"><a href="#决策树原理" class="headerlink" title="决策树原理"></a>决策树原理</h2><p>决策树作为一种机器学习算法，将 “特征a值小于x，特征b值小于y……&#x3D;&gt;类别1 “的逻辑规则流纳入树状数据结构。<strong>这种算法的优点是可解释性强</strong>。银行可以向客户解释他们被拒绝贷款的原因：例如，客户没有房子、收入低于5000元。</p>
<p>我们可以确保在前面的例子中建立的树是最佳的。在其他分割条件下，产生的树会更深，即需要更多的 “问题 “来达到答案。<br>决策树构建的流行算法，如ID3或C4.5，其核心是<strong>信息增益的贪婪最大化</strong>原则：在每一步，算法选择在分裂时提供最大信息增益的变量。然后递归地重复这个过程，直到熵为零（或某个小值以考虑过拟合的情况）。不同的算法使用不同的启发式 “早期停止 “或 “截止”，以避免构建一个过度拟合的树。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">build</span>(<span class="params">L</span>):</span><br><span class="line">    <span class="comment"># 先创建节点</span></span><br><span class="line">    create node t</span><br><span class="line">    <span class="comment"># 如果满足停止条件，则称`t`为所需的模型；否则寻找使熵最低的分割点`L`并对两侧建树</span></span><br><span class="line">    <span class="keyword">if</span> the stopping criterion <span class="keyword">is</span> <span class="literal">True</span>:</span><br><span class="line">        assign a predictive model to t</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        Find the best binary split L = L_left + L_right</span><br><span class="line">        t.left = build(L_left)</span><br><span class="line">        t.right = build(L_right)</span><br><span class="line">    <span class="keyword">return</span> t   </span><br></pre></td></tr></table></figure>

<p>步骤如下：</p>
<ol>
<li>自顶向下的贪婪搜索遍历可能的决策树空间构造决策树；</li>
<li>从“哪一个属性将在树的根节点被测试”开始；</li>
<li>使用统计测试来<strong>确定每一个实例属性单独分类训练样例的能力</strong>，分类能力最好的属性作为树的根结点测试。</li>
<li>然后为根结点属性的每个可能值产生一个分支，并把训练样例排列到适当的分支（也就是说，样例的该属性值对应的分支）之下。</li>
<li>重复这个过程，用每个分支结点关联的训练样例来选取在该点被测试的最佳属性。</li>
</ol>
<h3 id="熵"><a href="#熵" class="headerlink" title="熵"></a>熵</h3><p>对于具有 N 个可能状态的系统，香农熵定义如下：</p>
<p>$$<br>\Large Entropy(S)&#x3D; -\sum_{i&#x3D;1}^{N}p_i \log_2{p_i},</p>
<p>$$</p>
<p>其中 $p_i$ 是使系统处于第 $i$ 个状态的概率。这是物理学、信息论和其他领域中使用的一个非常重要的概念。熵可以描述为系统中的混沌程度：熵越高，系统的有序性越低，反之亦然。这将帮助我们进行有效的数据拆分。</p>
<p>当只有两种样例时，定义可写为：</p>
<p>$$<br>\Large Entropy(S)&#x3D; -p_{pos} \log_2{p_{pos}} -p_{neg} \log_2{p_{neg}}</p>
<p>$$</p>
<p>其C++实现如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//根据具体属性和值来计算熵  </span></span><br><span class="line"><span class="function"><span class="type">double</span> <span class="title">ComputeEntropy</span><span class="params">(vector &lt;vector &lt;string&gt; &gt; remain_state, string attribute, string value,<span class="type">bool</span> ifparent)</span></span>&#123;  </span><br><span class="line">    <span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">count</span> <span class="params">(<span class="number">2</span>,<span class="number">0</span>)</span></span>;  </span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> i,j;  </span><br><span class="line">    <span class="type">bool</span> done_flag = <span class="literal">false</span>;<span class="comment">//哨兵值  </span></span><br><span class="line">    <span class="keyword">for</span>(j = <span class="number">1</span>; j &lt; MAXLEN; j++)&#123;  </span><br><span class="line">        <span class="keyword">if</span>(done_flag) <span class="keyword">break</span>;  </span><br><span class="line">        <span class="keyword">if</span>(!attribute_row[j].<span class="built_in">compare</span>(attribute))&#123;  </span><br><span class="line">            <span class="keyword">for</span>(i = <span class="number">1</span>; i &lt; remain_state.<span class="built_in">size</span>(); i++)&#123;  </span><br><span class="line">                <span class="keyword">if</span>((!ifparent&amp;&amp;!remain_state[i][j].<span class="built_in">compare</span>(value)) || ifparent)&#123;<span class="comment">//ifparent记录是否算父节点  </span></span><br><span class="line">                    <span class="keyword">if</span>(!remain_state[i][MAXLEN - <span class="number">1</span>].<span class="built_in">compare</span>(yes))&#123;  </span><br><span class="line">                        count[<span class="number">0</span>]++;  </span><br><span class="line">                    &#125;  </span><br><span class="line">                    <span class="keyword">else</span> count[<span class="number">1</span>]++;  </span><br><span class="line">                &#125;  </span><br><span class="line">            &#125;  </span><br><span class="line">            done_flag = <span class="literal">true</span>;  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">if</span>(count[<span class="number">0</span>] == <span class="number">0</span> || count[<span class="number">1</span>] == <span class="number">0</span> ) <span class="keyword">return</span> <span class="number">0</span>;<span class="comment">//全部是正实例或者负实例  </span></span><br><span class="line">    <span class="comment">//具体计算熵 根据[+count[0],-count[1]],log2为底通过换底公式换成自然数底数  </span></span><br><span class="line">    <span class="type">double</span> sum = count[<span class="number">0</span>] + count[<span class="number">1</span>];  </span><br><span class="line">    <span class="type">double</span> entropy = -count[<span class="number">0</span>]/sum*<span class="built_in">log</span>(count[<span class="number">0</span>]/sum)/<span class="built_in">log</span>(<span class="number">2.0</span>) - count[<span class="number">1</span>]/sum*<span class="built_in">log</span>(count[<span class="number">1</span>]/sum)/<span class="built_in">log</span>(<span class="number">2.0</span>);  </span><br><span class="line">    <span class="keyword">return</span> entropy;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h3><p>$$<br>\Large IG(Q) &#x3D; S_O - \sum_{i&#x3D;1}^{q}\frac{N_i}{N}S_i</p>
<p>$$</p>
<h3 id="分类问题中的评判标准"><a href="#分类问题中的评判标准" class="headerlink" title="分类问题中的评判标准"></a>分类问题中的评判标准</h3><p>我们讨论了熵，但还有其他的评判标准：</p>
<ul>
<li>基尼不确定性 Gini uncertainty（基尼杂质Gini impurity）： $G &#x3D; 1 - \sum\limits_k (p_k)^2$. 最大化基尼不确定性可以最大化同一子树中同一类的对象对的数量。</li>
<li>Misclassification error (误判误差):  $E &#x3D; 1 - \max\limits_k p_k$</li>
</ul>
<p>实践中几乎从不使用误判误差，基尼不确定性和信息增益效果类似。</p>
<p>对于二元分类，熵和基尼不确定性采用以下形式：</p>
<p>$ S &#x3D; -p_+ \log_2{p_+} -p_- \log_2{p_-} &#x3D; -p_+ \log_2{p_+} -(1 - p_{+}) \log_2{(1 - p_{+})};$</p>
<p>$ G &#x3D; 1 - p_+^2 - p_-^2 &#x3D; 1 - p_+^2 - (1 - p_+)^2 &#x3D; 2p_+(1-p_+).$</p>
<p>其中$p_+$ 是对象具有标签 <code>+</code> 的概率。</p>
<p>如果我们根据 $p_+$ 绘制这两个函数，就能发现看到熵是基尼不确定性的两倍。因此实践中这两个标准几乎相同。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 消除警告</span></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&quot;ignore&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line">sns.<span class="built_in">set</span>()</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">%config InlineBackend.figure_format = <span class="string">&#x27;retina&#x27;</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">6</span>, <span class="number">4</span>))</span><br><span class="line">xx = np.linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="number">50</span>)</span><br><span class="line">plt.plot(xx, [<span class="number">2</span> * x * (<span class="number">1</span> - x) <span class="keyword">for</span> x <span class="keyword">in</span> xx], label=<span class="string">&quot;gini&quot;</span>)</span><br><span class="line">plt.plot(xx, [<span class="number">4</span> * x * (<span class="number">1</span> - x) <span class="keyword">for</span> x <span class="keyword">in</span> xx], label=<span class="string">&quot;2*gini&quot;</span>)</span><br><span class="line">plt.plot(xx, [-x * np.log2(x) - (<span class="number">1</span> - x) * np.log2(<span class="number">1</span> - x) <span class="keyword">for</span> x <span class="keyword">in</span> xx], label=<span class="string">&quot;entropy&quot;</span>)</span><br><span class="line">plt.plot(xx, [<span class="number">1</span> - <span class="built_in">max</span>(x, <span class="number">1</span> - x) <span class="keyword">for</span> x <span class="keyword">in</span> xx], label=<span class="string">&quot;missclass&quot;</span>)</span><br><span class="line">plt.plot(xx, [<span class="number">2</span> - <span class="number">2</span> * <span class="built_in">max</span>(x, <span class="number">1</span> - x) <span class="keyword">for</span> x <span class="keyword">in</span> xx], label=<span class="string">&quot;2*missclass&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;p+&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;criterion&quot;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Criteria of quality as a function of p+ (binary classification)&quot;</span>)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure>

<h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><p>优点：</p>
<ul>
<li>可解释性强</li>
<li>能被可视化</li>
<li>训练和预测速度快</li>
<li>模型参数少</li>
<li>支持数字特征和分类特征都支持</li>
</ul>
<p>缺点：</p>
<ul>
<li>对数据中的噪声敏感；稍微修改训练集（如删除一个特征，添加一些对象），整个模型可能会发生变化</li>
<li>由决策树构建的分离边界有其局限性——它由垂直于坐标轴之一的超平面组成，在实践中，其质量不如其他一些方法。</li>
<li>会过拟合</li>
<li>不稳定。数据的微小变化能显著改变决策树。这个问题通过决策树集成来解决（下一次讨论）。</li>
<li>最优决策树搜索问题是 NP 完备的。在实践中使用了如贪婪搜索具有最大信息增益的特征，但并不能保证找到全局最优树。</li>
<li>难以应对数据中的缺失值。</li>
<li>模型只能内插不能外推（随机森林和树增强也如此）。决策树对特征空间外的对象预测结果相同。</li>
</ul>
<h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><h3 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h3><p>为人造数据拟合决策树。先生成两类服从不同均值的正态分布样本。</p>
<h4 id="数据生成"><a href="#数据生成" class="headerlink" title="数据生成"></a>数据生成</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一类</span></span><br><span class="line">np.random.seed(<span class="number">17</span>)</span><br><span class="line">train_data = np.random.normal(size=(<span class="number">100</span>, <span class="number">2</span>))</span><br><span class="line">train_labels = np.zeros(<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二类</span></span><br><span class="line">train_data = np.r_[train_data, np.random.normal(size=(<span class="number">100</span>, <span class="number">2</span>), loc=<span class="number">2</span>)]</span><br><span class="line">train_labels = np.r_[train_labels, np.ones(<span class="number">100</span>)]</span><br></pre></td></tr></table></figure>

<h4 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h4><p>此分类问题是建立边界来分隔两个类（红点和黄点）。一条直线太简单，为每个红点蜿蜒的复杂曲线太复杂，会导致我们在新样本上出错。直观地说，平滑的边界或一条直线或超平面可以很好地处理新数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">8</span>))</span><br><span class="line">plt.scatter(</span><br><span class="line">    train_data[:, <span class="number">0</span>],</span><br><span class="line">    train_data[:, <span class="number">1</span>],</span><br><span class="line">    c=train_labels,</span><br><span class="line">    cmap=<span class="string">&quot;Set1&quot;</span>,</span><br><span class="line">    linewidth=<span class="number">1.5</span>,</span><br><span class="line">)</span><br><span class="line">plt.plot(<span class="built_in">range</span>(-<span class="number">2</span>, <span class="number">5</span>), <span class="built_in">range</span>(<span class="number">4</span>, -<span class="number">3</span>, -<span class="number">1</span>));</span><br></pre></td></tr></table></figure>

<h4 id="模型搭建与训练"><a href="#模型搭建与训练" class="headerlink" title="模型搭建与训练"></a>模型搭建与训练</h4><p>训练<code>Sklearn</code>决策树来区分这两个类，使用<code>max_depth</code>限制树的深度并可视化生成的分离边界。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"></span><br><span class="line">clf_tree = DecisionTreeClassifier(criterion=<span class="string">&quot;entropy&quot;</span>, max_depth=<span class="number">3</span>, random_state=<span class="number">17</span>)</span><br><span class="line"></span><br><span class="line">clf_tree.fit(train_data, train_labels)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 画出边界</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_grid</span>(<span class="params">data</span>):</span><br><span class="line">    x_min, x_max = data[:, <span class="number">0</span>].<span class="built_in">min</span>() - <span class="number">1</span>, data[:, <span class="number">0</span>].<span class="built_in">max</span>() + <span class="number">1</span></span><br><span class="line">    y_min, y_max = data[:, <span class="number">1</span>].<span class="built_in">min</span>() - <span class="number">1</span>, data[:, <span class="number">1</span>].<span class="built_in">max</span>() + <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> np.meshgrid(np.arange(x_min, x_max, <span class="number">0.01</span>), np.arange(y_min, y_max, <span class="number">0.01</span>))</span><br><span class="line"></span><br><span class="line">xx, yy = get_grid(train_data)</span><br><span class="line">predicted = clf_tree.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)</span><br><span class="line">plt.pcolormesh(xx, yy, predicted, cmap=<span class="string">&quot;Set1&quot;</span>)</span><br><span class="line">plt.scatter(train_data[:, <span class="number">0</span>], train_data[:, <span class="number">1</span>], c=train_labels,</span><br><span class="line">    cmap=<span class="string">&quot;Set1&quot;</span>, edgecolors=<span class="string">&quot;black&quot;</span>, linewidth=<span class="number">1.5</span>);</span><br></pre></td></tr></table></figure>

<h4 id="树模型可视化"><a href="#树模型可视化" class="headerlink" title="树模型可视化"></a>树模型可视化</h4><p>这个代码很难在<code>kaggle</code>上跑通……直接放图</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !pip install pydotplus</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pydotplus</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> export_graphviz</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tree_graph_to_png</span>(<span class="params">tree, feature_names, png_file_to_save</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    This requires GraphViz to be installed.  </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">  </span><br><span class="line">    tree_str = export_graphviz(</span><br><span class="line">        tree, feature_names=feature_names, filled=<span class="literal">True</span>, out_file=<span class="literal">None</span></span><br><span class="line">    )</span><br><span class="line">    graph = pydotplus.graph_from_dot_data(tree_str)</span><br><span class="line">    graph.write_png(png_file_to_save)</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">tree_graph_to_png(</span><br><span class="line">    tree=clf_tree,</span><br><span class="line">    feature_names=[<span class="string">&quot;x1&quot;</span>, <span class="string">&quot;x2&quot;</span>],</span><br><span class="line">    png_file_to_save=<span class="string">&quot;topic3_decision_tree1.png&quot;</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<img src="https://habrastorage.org/webt/z0/yp/wm/z0ypwmvjko4r2mymcohy7ovevwe.png" />

<p>一开始有200个样本，每类100个。初始状态的熵最大，$S&#x3D;1$。然后，通过将 $x_2$ 的值与 1.211 进行比较，将样本第一次分成 2 组，左右组的熵都随之减少。在上面这个可视化图片中，第一类的样本越多橙色越深，第二类的样本越多蓝色越深。一开始，两个类的样本数量相等，所以树的根节点是白色的。</p>
<p>理论上您可以构建一棵决策树直到每个叶子都只有一个实例，但这在构建单棵树时并不常见，因为它会过拟合或泛化能力降低，树底部将是一些不重要的特征。</p>
<p>以下两种情况可将树构建到最大深度：</p>
<ul>
<li>随机森林。随机森林（一组树）平均单个树的输出。</li>
<li>修剪枝叶 (Pruning trees)。首先将树构造到最大深度,然后自下而上地通过比较具有和不具有该节点的树的质量来删除树的一些节点（使用交叉验证比较）。</li>
</ul>
<p>下图是在过拟合树中构建的分割边界示例。</p>
<img align='center' src='https://habrastorage.org/files/f9f/3b5/133/f9f3b5133bae460ba96ab7e546155b1d.png'>

<h3 id="回归问题"><a href="#回归问题" class="headerlink" title="回归问题"></a>回归问题</h3><p>预测数值型变量时，标准会发生变化：</p>
<ul>
<li>Variance:</li>
</ul>
<p>$$<br>\Large D &#x3D; \frac{1}{\ell} \sum\limits_{i &#x3D;1}^{\ell} (y_i - \frac{1}{\ell} \sum\limits_{j&#x3D;1}^{\ell} y_j)^2,</p>
<p>$$</p>
<p>we look for features that divide the training set in such a way that the values of the target feature in each leaf are roughly equal.其中$\ell$是一个叶子的样本数，$y_i$是目标变量的值。简单地说，我们通过最小化方差寻找的特征是以这样一种方式划分训练集，即目标特征在每个叶子中的值大致相等。</p>
<p>让我们生成一些服从 $f(x) &#x3D; e^{-x ^ 2} + 1.5 * e^{-(x - 2) ^ 2}$ 的数据，并加上一些噪声。然后我们将使用这些数据和树所做的预测来训练一棵树。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">n_train = <span class="number">150</span></span><br><span class="line">n_test = <span class="number">1000</span></span><br><span class="line">noise = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">    x = x.ravel()</span><br><span class="line">    <span class="keyword">return</span> np.exp(-(x ** <span class="number">2</span>)) + <span class="number">1.5</span> * np.exp(-((x - <span class="number">2</span>) ** <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params">n_samples, noise</span>):</span><br><span class="line">    X = np.random.rand(n_samples) * <span class="number">10</span> - <span class="number">5</span></span><br><span class="line">    X = np.sort(X).ravel()</span><br><span class="line">    y = (</span><br><span class="line">        np.exp(-(X ** <span class="number">2</span>))</span><br><span class="line">        + <span class="number">1.5</span> * np.exp(-((X - <span class="number">2</span>) ** <span class="number">2</span>))</span><br><span class="line">        + np.random.normal(<span class="number">0.0</span>, noise, n_samples)</span><br><span class="line">    )</span><br><span class="line">    X = X.reshape((n_samples, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> X, y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">X_train, y_train = generate(n_samples=n_train, noise=noise)</span><br><span class="line">X_test, y_test = generate(n_samples=n_test, noise=noise)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line"></span><br><span class="line">reg_tree = DecisionTreeRegressor(max_depth=<span class="number">5</span>, random_state=<span class="number">17</span>)</span><br><span class="line"></span><br><span class="line">reg_tree.fit(X_train, y_train)</span><br><span class="line">reg_tree_pred = reg_tree.predict(X_test)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line">plt.plot(X_test, f(X_test), <span class="string">&quot;b&quot;</span>)</span><br><span class="line">plt.scatter(X_train, y_train, c=<span class="string">&quot;b&quot;</span>, s=<span class="number">20</span>)</span><br><span class="line">plt.plot(X_test, reg_tree_pred, <span class="string">&quot;g&quot;</span>, lw=<span class="number">2</span>)</span><br><span class="line">plt.xlim([-<span class="number">5</span>, <span class="number">5</span>])</span><br><span class="line">plt.title(</span><br><span class="line">    <span class="string">&quot;Decision tree regressor, MSE = %.2f&quot;</span></span><br><span class="line">    % (np.<span class="built_in">sum</span>((y_test - reg_tree_pred) ** <span class="number">2</span>) / n_test)</span><br><span class="line">)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>不难发现，决策树用分段常数函数逼近数据集</p>
<h3 id="使用决策树进行-MNIST-手写数字识别"><a href="#使用决策树进行-MNIST-手写数字识别" class="headerlink" title="使用决策树进行 MNIST 手写数字识别"></a>使用决策树进行 MNIST 手写数字识别</h3><h4 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line"></span><br><span class="line">data = load_digits()</span><br><span class="line">X, y = data.data, data.target</span><br><span class="line"></span><br><span class="line">X[<span class="number">0</span>, :].reshape([<span class="number">8</span>, <span class="number">8</span>])</span><br></pre></td></tr></table></figure>

<h4 id="MNIST-可视化"><a href="#MNIST-可视化" class="headerlink" title="MNIST 可视化"></a>MNIST 可视化</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">f, axes = plt.subplots(<span class="number">1</span>, <span class="number">4</span>, sharey=<span class="literal">True</span>, figsize=(<span class="number">16</span>, <span class="number">6</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">    axes[i].imshow(X[i, :].reshape([<span class="number">8</span>, <span class="number">8</span>]), cmap=<span class="string">&quot;Greys&quot;</span>);</span><br></pre></td></tr></table></figure>

<h4 id="数据集划分"><a href="#数据集划分" class="headerlink" title="数据集划分"></a>数据集划分</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_holdout, y_train, y_holdout = train_test_split(</span><br><span class="line">    X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">17</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h4 id="模型建立与训练"><a href="#模型建立与训练" class="headerlink" title="模型建立与训练"></a>模型建立与训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tree = DecisionTreeClassifier(max_depth=<span class="number">5</span>, random_state=<span class="number">17</span>)</span><br><span class="line"></span><br><span class="line">tree.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>

<h4 id="模型预测与评定"><a href="#模型预测与评定" class="headerlink" title="模型预测与评定"></a>模型预测与评定</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tree_pred = tree.predict(X_holdout)</span><br><span class="line"></span><br><span class="line">accuracy_score(y_holdout, tree_pred)</span><br></pre></td></tr></table></figure>

<h4 id="调参"><a href="#调参" class="headerlink" title="调参"></a>调参</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tree_params = &#123;</span><br><span class="line">    <span class="string">&quot;max_depth&quot;</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">25</span>, <span class="number">30</span>, <span class="number">40</span>, <span class="number">50</span>, <span class="number">64</span>],</span><br><span class="line">    <span class="string">&quot;max_features&quot;</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">50</span>, <span class="number">64</span>],</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">tree_grid = GridSearchCV(tree, tree_params, cv=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, verbose=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">tree_grid.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>

<p>显示最优参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tree_grid.best_params_, tree_grid.best_score_  <span class="comment"># (&#123;&#x27;max_depth&#x27;: 20, &#x27;max_features&#x27;: 64&#125;, 0.844)</span></span><br></pre></td></tr></table></figure>

<h4 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">np.mean(</span><br><span class="line">    cross_val_score(RandomForestClassifier(random_state=<span class="number">17</span>), X_train, y_train, cv=<span class="number">5</span>)</span><br><span class="line">)  <span class="comment"># 0.935</span></span><br></pre></td></tr></table></figure>

<h2 id="调库"><a href="#调库" class="headerlink" title="调库"></a>调库</h2><h3 id="sklearn-tree-DecisionTreeClassifier"><a href="#sklearn-tree-DecisionTreeClassifier" class="headerlink" title="sklearn.tree.DecisionTreeClassifier"></a>sklearn.tree.DecisionTreeClassifier</h3><p><a target="_blank" rel="noopener" href="http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"><code>sklearn.tree.DecisionTreeClassifier</code></a> 类的主要参数有:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">sklearn</span>.tree.DecisionTreeClassifier(*, criterion=<span class="string">&#x27;gini&#x27;</span>, splitter=<span class="string">&#x27;best&#x27;</span>, max_depth=<span class="literal">None</span>, min_samples_split=<span class="number">2</span>, min_samples_leaf=<span class="number">1</span>, min_weight_fraction_leaf=<span class="number">0.0</span>, max_features=<span class="literal">None</span>, random_state=<span class="literal">None</span>, max_leaf_nodes=<span class="literal">None</span>, min_impurity_decrease=<span class="number">0.0</span>, min_impurity_split=<span class="literal">None</span>, class_weight=<span class="literal">None</span>, ccp_alpha=<span class="number">0.0</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li><code>max_depth</code> – the maximum depth of the tree;</li>
<li><code>max_features</code> - the maximum number of features with which to search for the best partition (this is necessary with a large number of features because it would be “expensive” to search for partitions for <em>all</em> features);</li>
<li><code>min_samples_leaf</code> – the minimum number of samples in a leaf. This parameter prevents creating trees where any leaf would have only a few members.</li>
</ul>
<h4 id="Classifier可选参数"><a href="#Classifier可选参数" class="headerlink" title="Classifier可选参数"></a>Classifier可选参数</h4><p>参数：</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td><strong>criterion</strong></td>
<td><strong>{“gini”, “entropy”}, default&#x3D;”gini”</strong><br/>这个参数是用来选择使用何种方法度量树的切分质量的。当criterion取值为“gini”时采用 基尼不纯度（Gini impurity）算法构造决策树，当criterion取值为 “entropy” 时采用信息增益（ information gain）算法构造决策树.</td>
</tr>
<tr>
<td><strong>splitter</strong></td>
<td><strong>{“best”, “random”}, default&#x3D;”best”</strong><br/>此参数决定了在每个节点上拆分策略的选择。支持的策略是“best” 选择“最佳拆分策略”， “random” 选择“最佳随机拆分策略”。</td>
</tr>
<tr>
<td><strong>max_depth</strong></td>
<td><strong>int, default&#x3D;None</strong><br/>树的最大深度。如果取值为None,则将所有节点展开，直到所有的叶子都是纯净的或者直到所有叶子都包含少于min_samples_split个样本。</td>
</tr>
<tr>
<td><strong>min_samples_split</strong></td>
<td><strong>int or float, default&#x3D;2</strong><br/>拆分内部节点所需的最少样本数：<br/><br/><strong>·</strong> 如果取值 int , 则将<code>min_samples_split</code>视为最小值。<br/><strong>·</strong> 如果为float，则<code>min_samples_split</code>是一个分数，而<code>ceil（min_samples_split * n_samples）</code>是每个拆分的最小样本数。<br/><br/>-注释 在版本0.18中更改：增加了分数形式的浮点值。<br/></td>
</tr>
<tr>
<td><strong>min_samples_leaf</strong></td>
<td><strong>int or float, default&#x3D;1</strong><br/>在叶节点处所需的最小样本数。 仅在任何深度的分裂点在左分支和右分支中的每个分支上至少留有<code>min_samples_leaf</code>个训练样本时，才考虑。 这可能具有平滑模型的效果，尤其是在回归中。<br/><br/><strong>·</strong> 如果为int，则将<code>min_samples_leaf</code>视为最小值<br/><strong>·</strong> 如果为float，则<code>min_samples_leaf</code>是一个分数，而<code>ceil（min_samples_leaf * n_samples）</code>是每个节点的最小样本数。<br/><br/>- 注释： 在版本0.18中发生了更改：添加了分数形式的浮点值。</td>
</tr>
<tr>
<td><strong>min_weight_fraction_leaf</strong></td>
<td><strong>float, default&#x3D;0.0</strong><br/>在所有叶节点处（所有输入样本）的权重总和中的最小加权分数。 如果未提供<code>sample_weight</code>，则样本的权重相等。</td>
</tr>
<tr>
<td><strong>max_features</strong></td>
<td><strong>int, float or {“auto”, “sqrt”, “log2”}, default&#x3D;None</strong><br/>寻找最佳分割时要考虑的特征数量：<br/><strong>-</strong> 如果为<code>int</code>，则在每次拆分时考虑<code>max_features</code>功能。<br/><strong>-</strong> 如果为<code>float</code>，则<code>max_features</code>是一个分数，而<code>int（max_features * n_features）</code>是每个分割处的特征数量。<br/><strong>-</strong> 如果为<code>“auto”</code>，则<code>max_features = sqrt（n_features）</code>。<br/><strong>-</strong> 如果为<code>“sqrt”</code>，则<code>max_features = sqrt（n_features）</code>。<br/><strong>-</strong> 如果为<code>“log2”</code>，则<code>max_features = log2（n_features）</code>。<br/><strong>-</strong> 如果为<code>None</code>，则<code>max_features = n_features</code>。<br/><br/>注意：直到找到至少一个有效的节点样本分区，分割的搜索才会停止，即使它需要有效检查的特征数量多于<code>max_features</code>也是如此。</td>
</tr>
<tr>
<td><strong>random_state</strong></td>
<td><strong>int, RandomState instance, default&#x3D;None</strong><br/>此参数用来控制估计器的随机性。即使分割器设置为“最佳”，这些特征也总是在每个分割中随机排列。当<code>max_features &lt;n_features</code>时，该算法将在每个拆分中随机选择<code>max_features</code>，然后再在其中找到最佳拆分。但是，即使<code>max_features = n_features</code>，找到的最佳分割也可能因不同的运行而有所不同。 就是这种情况，如果标准的改进对于几个拆分而言是相同的，并且必须随机选择一个拆分。 为了在拟合过程中获得确定性的行为，<code>random_state</code>必须固定为整数。 有关详细信息，请参见词汇表。</td>
</tr>
<tr>
<td><strong>max_leaf_nodes</strong></td>
<td><strong>int, default&#x3D;None</strong><br/>优先以最佳方式生成带有<code>max_leaf_nodes</code>的树。 最佳节点定义为不纯度的相对减少。 如果为None，则叶节点数不受限制。</td>
</tr>
<tr>
<td><strong>min_impurity_decrease</strong></td>
<td><strong>float, default&#x3D;0.0</strong><br/>如果节点分裂会导致不纯度的减少大于或等于该值，则该节点将被分裂。<br/><br/>加权不纯度减少方程如下：<br/><code>N_t / N * (impurity - N_t_R / N_t * right_impurity</code><br/><code>- N_t_L / N_t * left_impurity)</code><br/>其中<code>N</code>是样本总数，<code>N_t</code>是当前节点上的样本数，<code>N_t_L</code>是左子节点中的样本数，<code>N_t_R</code>是右子节点中的样本数。<br/><br/>如果给<code>sample_weight</code>传了值，则<code>N , N_t , N_t_R</code> 和 <code>N_t_L</code>均指加权总和。<br/><br/>在 0.19 版新增 。</td>
</tr>
<tr>
<td><strong>min_impurity_split</strong></td>
<td><strong>float, default&#x3D;0</strong><br/>树模型停止生长的阈值。如果节点的不纯度高于阈值，则该节点将分裂，否则为叶节点。<br/><br/>警告： 从版本0.19开始被弃用:<code>min_impurity_split</code>在0.19中被弃用，转而支持<code>min_impurity_decrease</code>。<code>min_impurity_split</code>的默认值在0.23中从<code>1e-7</code>更改为<code>0</code>，在0.25中将被删除。使用<code>min_impurity_decrease</code>代替。</td>
</tr>
<tr>
<td><strong>class_weight</strong></td>
<td><strong>dict, list of dict or “balanced”, default&#x3D;None</strong><br/>以<code>&#123;class_label: weight&#125;</code>的形式表示与类别关联的权重。如果取值None,所有分类的权重为1。对于多输出问题，可以按照y的列的顺序提供一个字典列表。<br/><br/>注意多输出(包括多标签) ，应在其自己的字典中为每一列的每个类别定义权重。例如：对于四分类多标签问题， 权重应为[{0：1、1：1：1]，{0：1、1：5}，{0：1、1：1：1}，{0：1、1： 1}]，而不是[{1：1}，{2：5}，{3：1}，{4：1}]。<br/><br/>“平衡”模式使用y的值自动将权重与输入数据中的类频率成反比地调整为<code>n_samples /（n_classes * np.bincount（y））</code>。<br/><br/>对于多输出，y的每一列的权重将相乘。<br/><br/>请注意，如果指定了<code>sample_weight</code>，则这些权重将与<code>sample_weight</code>（通过<code>fit</code>方法传递）相乘。</td>
</tr>
<tr>
<td><strong>presort</strong></td>
<td><strong>deprecated, default&#x3D;’deprecated’</strong><br/>此参数已弃用，并将在v0.24中删除。<br/><br/>注意：从0.22版开始已弃用。</td>
</tr>
<tr>
<td><strong>ccp_alpha</strong></td>
<td><strong>non-negative float, default&#x3D;0.0</strong><br/>用于最小化成本复杂性修剪的复杂性参数。 将选择成本复杂度最大且小于ccp_alpha的子树。 默认情况下，不执行修剪。 有关详细信息，请参见最小成本复杂性修剪。</td>
</tr>
</tbody></table>
<p>属性</p>
<table>
<thead>
<tr>
<th>属性</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td><strong>classes_</strong></td>
<td><strong>ndarray of shape (n_classes,) or list of ndarray</strong><br/>类标签（单输出问题）或类标签数组的列表（多输出问题）。</td>
</tr>
<tr>
<td><strong>feature_importances_</strong></td>
<td><strong>ndarray of shape (n_features,)</strong><br/>返回特征重要程度数据。</td>
</tr>
<tr>
<td><strong>max_features_</strong></td>
<td><strong>int</strong><br/><code>max_features</code> 的推断值。<br/></td>
</tr>
<tr>
<td><strong>n_classes_</strong></td>
<td><strong>int or list of int</strong><br/>整数的类别数（单输出问题），或者一个包含所有类别数量的列表（多输出问题）。</td>
</tr>
<tr>
<td><strong>n_features_</strong></td>
<td><strong>int</strong><br/>执行模型拟合训练时的特征数量。</td>
</tr>
<tr>
<td><strong>n_outputs_</strong></td>
<td><strong>int</strong><br/>执行模型拟合训练时的输出数量。</td>
</tr>
<tr>
<td><strong>tree_</strong></td>
<td><strong>Tree</strong><br/>基础的Tree对象。请通过 <code>help(sklearn.tree._tree.Tree)</code>查看Tree对象的属性，并<a target="_blank" rel="noopener" href="http://scikit-learn.org.cn/view/148.html">了解决策树的结构</a>以了解这些属性的基本用法。</td>
</tr>
</tbody></table>
<h4 id="Classifier使用示例"><a href="#Classifier使用示例" class="headerlink" title="Classifier使用示例"></a>Classifier使用示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line">clf = DecisionTreeClassifier(random_state=<span class="number">0</span>)</span><br><span class="line">iris = load_iris()</span><br><span class="line">cross_val_score(clf, iris.data, iris.target, cv=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<h3 id="sklearn-tree-DecisionTreeRegressor"><a href="#sklearn-tree-DecisionTreeRegressor" class="headerlink" title="sklearn.tree.DecisionTreeRegressor"></a>sklearn.tree.DecisionTreeRegressor</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">sklearn</span>.tree.DecisionTreeRegressor(*, criterion=<span class="string">&#x27;mse&#x27;</span>, splitter=<span class="string">&#x27;best&#x27;</span>, max_depth=<span class="literal">None</span>, min_samples_split=<span class="number">2</span>, min_samples_leaf=<span class="number">1</span>, min_weight_fraction_leaf=<span class="number">0.0</span>, max_features=<span class="literal">None</span>, random_state=<span class="literal">None</span>, max_leaf_nodes=<span class="literal">None</span>, min_impurity_decrease=<span class="number">0.0</span>, min_impurity_split=<span class="literal">None</span>, presort=<span class="string">&#x27;deprecated&#x27;</span>, ccp_alpha=<span class="number">0.0</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li><code>criterion</code>{“squared_error”, “friedman_mse”, “absolute_error”, “poisson”}, default&#x3D;”squared_error”<br>The function to measure the quality of a split. Supported criteria are “squared_error” for the mean squared error, which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node, “friedman_mse”, which uses mean squared error with Friedman’s improvement score for potential splits, “absolute_error” for the mean absolute error, which minimizes the L1 loss using the median of each terminal node, and “poisson” which uses reduction in Poisson deviance to find splits.</li>
</ul>
<h4 id="Regressor可选参数"><a href="#Regressor可选参数" class="headerlink" title="Regressor可选参数"></a>Regressor可选参数</h4><p>参数：</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td><strong>criterion</strong></td>
<td><strong>{“mse”, “friedman_mse”, “mae”}, default&#x3D;”mse”</strong><br/>测量分割质量的函数。支持的标准是均方误差的“ mse”，等于方差减少作为特征选择标准，并使用每个终端节点的均值“ friedman_mse”来最小化L2损失，该方法使用均方误差和弗里德曼改进分数作为潜在值拆分，并使用“ mae”表示平均绝对误差，使用每个终端节点的中值将L1损失最小化。<br/><em>版本0.18</em> 中的<em>新功能</em> ：平均绝对误差（MAE）标准。</td>
</tr>
<tr>
<td><strong>splitter</strong></td>
<td><strong>{“best”, “random”}, default&#x3D;”best”</strong><br/>用于在每个节点上选择拆分的策略。支持的策略是“best”选择最佳拆分，“random”选择最佳随机拆分。</td>
</tr>
<tr>
<td><strong>max_depth</strong></td>
<td><strong>int, default&#x3D;None</strong><br/>树的最大深度。如果为None，则将节点展开，直到所有叶子都是纯净的，或者直到所有叶子都包含少于min_samples_split个样本。</td>
</tr>
<tr>
<td><strong>min_samples_split</strong></td>
<td><strong>int or float, default&#x3D;2</strong><br/>拆分内部节点所需的最少样本数：<br/>- 如果为int，则认为<code>min_samples_split</code>是最小值。<br/>- 如果为float，<code>min_samples_split</code>则为分数， <code>ceil(min_samples_split * n_samples)</code>是每个拆分的最小样本数。<br/><em>在版本0.18中更改</em> ：添加了分数的浮点值。</td>
</tr>
<tr>
<td><strong>min_samples_leaf</strong></td>
<td><strong>int or float, default&#x3D;1</strong><br/>在叶节点处需要的最小样本数。任何深度的分割点只有在左、右分支中至少留下min_samples_leaf训练样本时才会被考虑。这可能具有平滑模型的效果，尤其是在回归中。</td>
</tr>
<tr>
<td><strong>min_weight_fraction_leaf</strong></td>
<td><strong>float, default&#x3D;0.0</strong><br/>在所有叶节点处（所有输入样本）的权重总和中的最小加权分数。如果未提供sample_weight，则样本的权重相等。</td>
</tr>
<tr>
<td><strong>max_features</strong></td>
<td><strong>int, float or {“auto”, “sqrt”, “log2”}, default&#x3D;None</strong><br/>寻找最佳分割时要考虑的特征数量：<br/>- 如果为int，则<code>max_features</code>在每个分割处考虑特征。<br/>- 如果为float，<code>max_features</code>则为小数，并且<code>int(max_features * n_features)</code>是在每次拆分时考虑要素。<br/>- 如果为“auto”，则为<code>max_features=n_features</code>。<br/>- 如果是“ sqrt”，则<code>max_features=sqrt(n_features)</code>。<br/>- 如果为“ log2”，则为<code>max_features=log2(n_features)</code>。<br/>- 如果为”None“，则<code>max_features=n_features</code>。<br/>注：直到找到至少一个有效的节点样本分区，分割的搜索才会停止，即使它需要有效检查多个<code>max_features</code>要素也是如此。</td>
</tr>
<tr>
<td><strong>random_state</strong></td>
<td><strong>int, RandomState instance, default&#x3D;None</strong><br/>控制估算器的随机性。即使<code>splitter</code>设置为<code>&quot;best&quot;</code>，这些特性总是在每次拆分时随机排列。当<code>max_features &lt; n_features</code>时,算法将在每个分割处随机选择<code>max_features</code>特征，然后在其中找到最佳分割。即使<code>max_features=n_features</code>，但最佳分割可能因不同的运行而有所不同，这种情况下，如果标准的改进对于多个分割是相同的，必须随机选择一个分割。为了在拟合过程中获得确定性行为，必须将其固定为整数。有关详细信息，请参见<a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/glossary.html#term-random-state">词汇表</a>。</td>
</tr>
<tr>
<td><strong>max_leaf_nodes</strong></td>
<td><strong>int, default&#x3D;None</strong><br/>以最好的方式种植一棵树，树上有<code>max_l</code>节点。最佳节点定义为杂质相对减少。如果没有，则叶节点的数量不受限制。</td>
</tr>
<tr>
<td><strong>min_impurity_decrease</strong></td>
<td><strong>float, default&#x3D;0.0</strong><br/>如果节点分裂会导致杂质的减少大于或等于该值，则该节点将被分裂。<br/>加权杂质减少方程如下：<br/><code>N_t / N * (impurity - N_t_R / N_t * right_impurity</code> <code>N_t_L / N_t * left_impurity)</code><br/>其中，<code>N</code>是样本总数，<code>N_t</code>是当前节点的样本数，<code>N_t_L</code>是左子节点中的样本数，<code>N_t_R</code>是右子节点中的样本数。<br/><code>N</code>，<code>N_t</code>，<code>N_t_R</code>和<code>N_t_L</code>都指的是加权和，如果<code>sample_weight</code>可以通过。<br/><em>版本0.19中的新功能。</em></td>
</tr>
<tr>
<td><strong>min_impurity_split</strong></td>
<td><strong>float, (default&#x3D;0)</strong><br/>尽早停止树的生长的阈值。如果节点的杂质高于阈值，则该节点将分裂，否则为叶。<br/><em>从版本0.19</em> <code>min_impurity_split</code>开始不推荐使用：在<em>版本0.19中</em> 不再推荐使用 <code>min_impurity_decrease</code>。 <code>min_impurity_split</code>的默认值在0.23中从1e-7更改为0，将会在0.25中删除。使用<code>min_impurity_decrease</code>将其代替。</td>
</tr>
<tr>
<td><strong>presort</strong></td>
<td><strong>deprecated, default&#x3D;’deprecated’</strong><br/>此参数已弃用，并将在v0.24中删除。<br/><code>从0.22版开始不推荐使用。</code></td>
</tr>
<tr>
<td><strong>ccp_alpha</strong></td>
<td><strong>non-negative float, default&#x3D;0.0</strong><br/>用于最小代价复杂度修剪的复杂度参数。选择代价复杂度最大且小于<code>ccp_alpha</code>的子树。默认情况下，不执行修剪。有关详细信息，请参见 <a target="_blank" rel="noopener" href="http://scikit-learn.org.cn/view/89.html#1.10.8%20%E6%9C%80%E5%B0%8F%E6%88%90%E6%9C%AC%E5%A4%8D%E6%9D%82%E5%BA%A6%E5%89%AA%E6%9E%9D">最小成本复杂性修剪</a>。<br/><em>0.22版中的新功能。</em></td>
</tr>
</tbody></table>
<p>属性：</p>
<table>
<thead>
<tr>
<th>属性</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td><strong>feature_importances_</strong></td>
<td><strong>ndarray of shape (n_features,)</strong><br/>返特征的重要性。</td>
</tr>
<tr>
<td><strong>max_features_</strong></td>
<td><strong>int</strong><br/>max_features的推断值。</td>
</tr>
<tr>
<td><strong>n_features_</strong></td>
<td><strong>int</strong><br/><code>fit</code>执行时的特征数量。</td>
</tr>
<tr>
<td><strong>n_outputs_</strong></td>
<td><strong>int</strong><br/><code>fit</code>执行时的输出数量。</td>
</tr>
<tr>
<td><strong>tree_</strong></td>
<td><strong>Tree</strong><br/>基础的Tree对象。有关树对象的属性请参阅<code>help(sklearn.tree._tree.Tree)</code>， 有关这些属性的基本用法，请参阅 <a target="_blank" rel="noopener" href="http://scikit-learn.org.cn/view/148.html">决策树结构</a>。</td>
</tr>
</tbody></table>
<h4 id="Regressor使用示例"><a href="#Regressor使用示例" class="headerlink" title="Regressor使用示例"></a>Regressor使用示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_diabetes</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line">X, y = load_diabetes(return_X_y=<span class="literal">True</span>)</span><br><span class="line">regressor = DecisionTreeRegressor(random_state=<span class="number">0</span>)</span><br><span class="line">cross_val_score(regressor, X, y, cv=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><h3 id="ID3-实现"><a href="#ID3-实现" class="headerlink" title="ID3 实现"></a>ID3 实现</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//计算信息增益，DFS构建决策树  </span></span><br><span class="line"><span class="comment">//current_node为当前的节点  </span></span><br><span class="line"><span class="comment">//remain_state为剩余待分类的样例  </span></span><br><span class="line"><span class="comment">//remian_attribute为剩余还没有考虑的属性  </span></span><br><span class="line"><span class="comment">//返回根结点指针  </span></span><br><span class="line"><span class="function">Node * <span class="title">BulidDecisionTreeDFS</span><span class="params">(Node * p, vector &lt;vector &lt;string&gt; &gt; remain_state, vector &lt;string&gt; remain_attribute)</span></span>&#123;  </span><br><span class="line">    <span class="comment">//if(remain_state.size() &gt; 0)&#123;  </span></span><br><span class="line">        <span class="comment">//printv(remain_state);  </span></span><br><span class="line">    <span class="comment">//&#125;  </span></span><br><span class="line">    <span class="keyword">if</span> (p == <span class="literal">NULL</span>)  </span><br><span class="line">        p = <span class="keyword">new</span> <span class="built_in">Node</span>();  </span><br><span class="line">    <span class="comment">//先看搜索到树叶的情况  </span></span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">AllTheSameLabel</span>(remain_state, yes))&#123;  </span><br><span class="line">        p-&gt;attribute = yes;  </span><br><span class="line">        <span class="keyword">return</span> p;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">if</span>(remain_attribute.<span class="built_in">size</span>() == <span class="number">0</span>)&#123;<span class="comment">//所有的属性均已经考虑完了,还没有分尽  </span></span><br><span class="line">        string label = <span class="built_in">MostCommonLabel</span>(remain_state);  </span><br><span class="line">        p-&gt;attribute = label;  </span><br><span class="line">        <span class="keyword">return</span> p;  </span><br><span class="line">    &#125;  </span><br><span class="line">  </span><br><span class="line">    <span class="type">double</span> max_gain = <span class="number">0</span>, temp_gain;  </span><br><span class="line">    vector &lt;string&gt;::iterator max_it;  </span><br><span class="line">    vector &lt;string&gt;::iterator it1;  </span><br><span class="line">    <span class="keyword">for</span>(it1 = remain_attribute.<span class="built_in">begin</span>(); it1 &lt; remain_attribute.<span class="built_in">end</span>(); it1++)&#123;  </span><br><span class="line">        temp_gain = <span class="built_in">ComputeGain</span>(remain_state, (*it1));  </span><br><span class="line">        <span class="keyword">if</span>(temp_gain &gt; max_gain) &#123;  </span><br><span class="line">            max_gain = temp_gain;  </span><br><span class="line">            max_it = it1;  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="comment">//下面根据max_it指向的属性来划分当前样例，更新样例集和属性集  </span></span><br><span class="line">    vector &lt;string&gt; new_attribute;  </span><br><span class="line">    vector &lt;vector &lt;string&gt; &gt; new_state;  </span><br><span class="line">    <span class="keyword">for</span>(vector &lt;string&gt;::iterator it2 = remain_attribute.<span class="built_in">begin</span>(); it2 &lt; remain_attribute.<span class="built_in">end</span>(); it2++)&#123;  </span><br><span class="line">        <span class="keyword">if</span>((*it2).<span class="built_in">compare</span>(*max_it)) new_attribute.<span class="built_in">push_back</span>(*it2);  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="comment">//确定了最佳划分属性，注意保存  </span></span><br><span class="line">    p-&gt;attribute = *max_it;  </span><br><span class="line">    vector &lt;string&gt; values = map_attribute_values[*max_it];  </span><br><span class="line">    <span class="type">int</span> attribue_num = <span class="built_in">FindAttriNumByName</span>(*max_it);  </span><br><span class="line">    new_state.<span class="built_in">push_back</span>(attribute_row);  </span><br><span class="line">    <span class="keyword">for</span>(vector &lt;string&gt;::iterator it3 = values.<span class="built_in">begin</span>(); it3 &lt; values.<span class="built_in">end</span>(); it3++)&#123;  </span><br><span class="line">        <span class="keyword">for</span>(<span class="type">unsigned</span> <span class="type">int</span> i = <span class="number">1</span>; i &lt; remain_state.<span class="built_in">size</span>(); i++)&#123;  </span><br><span class="line">            <span class="keyword">if</span>(!remain_state[i][attribue_num].<span class="built_in">compare</span>(*it3))&#123;  </span><br><span class="line">                new_state.<span class="built_in">push_back</span>(remain_state[i]);  </span><br><span class="line">            &#125;  </span><br><span class="line">        &#125;  </span><br><span class="line">        Node * new_node = <span class="keyword">new</span> <span class="built_in">Node</span>();  </span><br><span class="line">        new_node-&gt;arrived_value = *it3;  </span><br><span class="line">        <span class="keyword">if</span>(new_state.<span class="built_in">size</span>() == <span class="number">0</span>)&#123;<span class="comment">//表示当前没有这个分支的样例，当前的new_node为叶子节点  </span></span><br><span class="line">            new_node-&gt;attribute = <span class="built_in">MostCommonLabel</span>(remain_state);  </span><br><span class="line">        &#125;  </span><br><span class="line">        <span class="keyword">else</span>   </span><br><span class="line">            <span class="built_in">BulidDecisionTreeDFS</span>(new_node, new_state, new_attribute);  </span><br><span class="line">        <span class="comment">//递归函数返回时即回溯时需要1 将新结点加入父节点孩子容器 2清除new_state容器  </span></span><br><span class="line">        p-&gt;childs.<span class="built_in">push_back</span>(new_node);  </span><br><span class="line">        new_state.<span class="built_in">erase</span>(new_state.<span class="built_in">begin</span>()+<span class="number">1</span>,new_state.<span class="built_in">end</span>());<span class="comment">//注意先清空new_state中的前一个取值的样例，准备遍历下一个取值样例  </span></span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> p;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding=utf-8</span></span><br><span class="line"><span class="comment">#Author:Dodo</span></span><br><span class="line"><span class="comment">#Date:2018-11-21</span></span><br><span class="line"><span class="comment">#Email:lvtengchao@pku.edu.cn</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">数据集：Mnist</span></span><br><span class="line"><span class="string">训练集数量：60000</span></span><br><span class="line"><span class="string">测试集数量：10000</span></span><br><span class="line"><span class="string">------------------------------</span></span><br><span class="line"><span class="string">运行结果：ID3(未剪枝)</span></span><br><span class="line"><span class="string">    正确率：85.9%</span></span><br><span class="line"><span class="string">    运行时长：356s</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loadData</span>(<span class="params">fileName</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    加载文件</span></span><br><span class="line"><span class="string">    :param fileName:要加载的文件路径</span></span><br><span class="line"><span class="string">    :return: 数据集和标签集</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment">#存放数据及标记</span></span><br><span class="line">    dataArr = []; labelArr = []</span><br><span class="line">    <span class="comment">#读取文件</span></span><br><span class="line">    fr = <span class="built_in">open</span>(fileName)</span><br><span class="line">    <span class="comment">#遍历文件中的每一行</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        <span class="comment">#获取当前行，并按“，”切割成字段放入列表中</span></span><br><span class="line">        <span class="comment">#strip：去掉每行字符串首尾指定的字符（默认空格或换行符）</span></span><br><span class="line">        <span class="comment">#split：按照指定的字符将字符串切割成每个字段，返回列表形式</span></span><br><span class="line">        curLine = line.strip().split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">        <span class="comment">#将每行中除标记外的数据放入数据集中（curLine[0]为标记信息）</span></span><br><span class="line">        <span class="comment">#在放入的同时将原先字符串形式的数据转换为整型</span></span><br><span class="line">        <span class="comment">#此外将数据进行了二值化处理，大于128的转换成1，小于的转换成0，方便后续计算</span></span><br><span class="line">        dataArr.append([<span class="built_in">int</span>(<span class="built_in">int</span>(num) &gt; <span class="number">128</span>) <span class="keyword">for</span> num <span class="keyword">in</span> curLine[<span class="number">1</span>:]])</span><br><span class="line">        <span class="comment">#将标记信息放入标记集中</span></span><br><span class="line">        <span class="comment">#放入的同时将标记转换为整型</span></span><br><span class="line">        labelArr.append(<span class="built_in">int</span>(curLine[<span class="number">0</span>]))</span><br><span class="line">    <span class="comment">#返回数据集和标记</span></span><br><span class="line">    <span class="keyword">return</span> dataArr, labelArr</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">majorClass</span>(<span class="params">labelArr</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    找到当前标签集中占数目最大的标签</span></span><br><span class="line"><span class="string">    :param labelArr: 标签集</span></span><br><span class="line"><span class="string">    :return: 最大的标签</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment">#建立字典，用于不同类别的标签技术</span></span><br><span class="line">    classDict = &#123;&#125;</span><br><span class="line">    <span class="comment">#遍历所有标签</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(labelArr)):</span><br><span class="line">        <span class="comment">#当第一次遇到A标签时，字典内还没有A标签，这时候直接幅值加1是错误的，</span></span><br><span class="line">        <span class="comment">#所以需要判断字典中是否有该键，没有则创建，有就直接自增</span></span><br><span class="line">        <span class="keyword">if</span> labelArr[i] <span class="keyword">in</span> classDict.keys():</span><br><span class="line">            <span class="comment"># 若在字典中存在该标签，则直接加1</span></span><br><span class="line">            classDict[labelArr[i]] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment">#若无该标签，设初值为1，表示出现了1次了</span></span><br><span class="line">            classDict[labelArr[i]] = <span class="number">1</span></span><br><span class="line">    <span class="comment">#对字典依据值进行降序排序</span></span><br><span class="line">    classSort = <span class="built_in">sorted</span>(classDict.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment">#返回最大一项的标签，即占数目最多的标签</span></span><br><span class="line">    <span class="keyword">return</span> classSort[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calc_H_D</span>(<span class="params">trainLabelArr</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    计算数据集D的经验熵，参考公式5.7 经验熵的计算</span></span><br><span class="line"><span class="string">    :param trainLabelArr:当前数据集的标签集</span></span><br><span class="line"><span class="string">    :return: 经验熵</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment">#初始化为0</span></span><br><span class="line">    H_D = <span class="number">0</span></span><br><span class="line">    <span class="comment">#将当前所有标签放入集合中，这样只要有的标签都会在集合中出现，且出现一次。</span></span><br><span class="line">    <span class="comment">#遍历该集合就可以遍历所有出现过的标记并计算其Ck</span></span><br><span class="line">    <span class="comment">#这么做有一个很重要的原因：首先假设一个背景，当前标签集中有一些标记已经没有了，比如说标签集中</span></span><br><span class="line">    <span class="comment">#没有0（这是很正常的，说明当前分支不存在这个标签）。 式5.7中有一项Ck，那按照式中的针对不同标签k</span></span><br><span class="line">    <span class="comment">#计算Cl和D并求和时，由于没有0，那么C0=0，此时C0/D0=0,log2(C0/D0) = log2(0)，事实上0并不在log的</span></span><br><span class="line">    <span class="comment">#定义区间内，出现了问题</span></span><br><span class="line">    <span class="comment">#所以使用集合的方式先知道当前标签中都出现了那些标签，随后对每个标签进行计算，如果没出现的标签那一项就</span></span><br><span class="line">    <span class="comment">#不在经验熵中出现（未参与，对经验熵无影响），保证log的计算能一直有定义</span></span><br><span class="line">    trainLabelSet = <span class="built_in">set</span>([label <span class="keyword">for</span> label <span class="keyword">in</span> trainLabelArr])</span><br><span class="line">    <span class="comment">#遍历每一个出现过的标签</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> trainLabelSet:</span><br><span class="line">        <span class="comment">#计算|Ck|/|D|</span></span><br><span class="line">        <span class="comment">#trainLabelArr == i：当前标签集中为该标签的的位置</span></span><br><span class="line">        <span class="comment">#例如a = [1, 0, 0, 1], c = (a == 1): c == [True, false, false, True]</span></span><br><span class="line">        <span class="comment">#trainLabelArr[trainLabelArr == i]：获得为指定标签的样本</span></span><br><span class="line">        <span class="comment">#trainLabelArr[trainLabelArr == i].size：获得为指定标签的样本的大小，即标签为i的样本</span></span><br><span class="line">        <span class="comment">#数量，就是|Ck|</span></span><br><span class="line">        <span class="comment">#trainLabelArr.size：整个标签集的数量（也就是样本集的数量），即|D|</span></span><br><span class="line">        p = trainLabelArr[trainLabelArr == i].size / trainLabelArr.size</span><br><span class="line">        <span class="comment">#对经验熵的每一项累加求和</span></span><br><span class="line">        H_D += -<span class="number">1</span> * p * np.log2(p)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#返回经验熵</span></span><br><span class="line">    <span class="keyword">return</span> H_D</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calcH_D_A</span>(<span class="params">trainDataArr_DevFeature, trainLabelArr</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    计算经验条件熵</span></span><br><span class="line"><span class="string">    :param trainDataArr_DevFeature:切割后只有feature那列数据的数组</span></span><br><span class="line"><span class="string">    :param trainLabelArr: 标签集数组</span></span><br><span class="line"><span class="string">    :return: 经验条件熵</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment">#初始为0</span></span><br><span class="line">    H_D_A = <span class="number">0</span></span><br><span class="line">    <span class="comment">#在featue那列放入集合中，是为了根据集合中的数目知道该feature目前可取值数目是多少</span></span><br><span class="line">    trainDataSet = <span class="built_in">set</span>([label <span class="keyword">for</span> label <span class="keyword">in</span> trainDataArr_DevFeature])</span><br><span class="line"></span><br><span class="line">    <span class="comment">#对于每一个特征取值遍历计算条件经验熵的每一项</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> trainDataSet:</span><br><span class="line">        <span class="comment">#计算H(D|A)</span></span><br><span class="line">        <span class="comment">#trainDataArr_DevFeature[trainDataArr_DevFeature == i].size / trainDataArr_DevFeature.size:|Di| / |D|</span></span><br><span class="line">        <span class="comment">#calc_H_D(trainLabelArr[trainDataArr_DevFeature == i]):H(Di)</span></span><br><span class="line">        H_D_A += trainDataArr_DevFeature[trainDataArr_DevFeature == i].size / trainDataArr_DevFeature.size \</span><br><span class="line">                * calc_H_D(trainLabelArr[trainDataArr_DevFeature == i])</span><br><span class="line">    <span class="comment">#返回得出的条件经验熵</span></span><br><span class="line">    <span class="keyword">return</span> H_D_A</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calcBestFeature</span>(<span class="params">trainDataList, trainLabelList</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    计算信息增益最大的特征</span></span><br><span class="line"><span class="string">    :param trainDataList: 当前数据集</span></span><br><span class="line"><span class="string">    :param trainLabelList: 当前标签集</span></span><br><span class="line"><span class="string">    :return: 信息增益最大的特征及最大信息增益值</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment">#将数据集和标签集转换为数组形式</span></span><br><span class="line">    trainDataArr = np.array(trainDataList)</span><br><span class="line">    trainLabelArr = np.array(trainLabelList)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#获取当前特征数目，也就是数据集的横轴大小</span></span><br><span class="line">    featureNum = trainDataArr.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#初始化最大信息增益</span></span><br><span class="line">    maxG_D_A = -<span class="number">1</span></span><br><span class="line">    <span class="comment">#初始化最大信息增益的特征</span></span><br><span class="line">    maxFeature = -<span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#“5.2.2 信息增益”中“算法5.1（信息增益的算法）”第一步：</span></span><br><span class="line">    <span class="comment">#1.计算数据集D的经验熵H(D)</span></span><br><span class="line">    H_D = calc_H_D(trainLabelArr)</span><br><span class="line">    <span class="comment">#对每一个特征进行遍历计算</span></span><br><span class="line">    <span class="keyword">for</span> feature <span class="keyword">in</span> <span class="built_in">range</span>(featureNum):</span><br><span class="line">        <span class="comment">#2.计算条件经验熵H(D|A)</span></span><br><span class="line">        <span class="comment">#由于条件经验熵的计算过程中只涉及到标签以及当前特征，为了提高运算速度（全部样本</span></span><br><span class="line">        <span class="comment">#做成的矩阵运算速度太慢，需要剔除不需要的部分），将数据集矩阵进行切割</span></span><br><span class="line">        <span class="comment">#数据集在初始时刻是一个Arr = 60000*784的矩阵，针对当前要计算的feature，在训练集中切割下</span></span><br><span class="line">        <span class="comment">#Arr[:, feature]这么一条来，因为后续计算中数据集中只用到这个（没明白的跟着算一遍例5.2）</span></span><br><span class="line">        <span class="comment">#trainDataArr[:, feature]:在数据集中切割下这么一条</span></span><br><span class="line">        <span class="comment">#trainDataArr[:, feature].flat：将这么一条转换成竖着的列表</span></span><br><span class="line">        <span class="comment">#np.array(trainDataArr[:, feature].flat)：再转换成一条竖着的矩阵，大小为60000*1（只是初始是</span></span><br><span class="line">        <span class="comment">#这么大，运行过程中是依据当前数据集大小动态变的）</span></span><br><span class="line">        trainDataArr_DevideByFeature = np.array(trainDataArr[:, feature].flat)</span><br><span class="line">        <span class="comment">#3.计算信息增益G(D|A)    G(D|A) = H(D) - H(D | A)</span></span><br><span class="line">        G_D_A = H_D - calcH_D_A(trainDataArr_DevideByFeature, trainLabelArr)</span><br><span class="line">        <span class="comment">#不断更新最大的信息增益以及对应的feature</span></span><br><span class="line">        <span class="keyword">if</span> G_D_A &gt; maxG_D_A:</span><br><span class="line">            maxG_D_A = G_D_A</span><br><span class="line">            maxFeature = feature</span><br><span class="line">    <span class="keyword">return</span> maxFeature, maxG_D_A</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getSubDataArr</span>(<span class="params">trainDataArr, trainLabelArr, A, a</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    更新数据集和标签集</span></span><br><span class="line"><span class="string">    :param trainDataArr:要更新的数据集</span></span><br><span class="line"><span class="string">    :param trainLabelArr: 要更新的标签集</span></span><br><span class="line"><span class="string">    :param A: 要去除的特征索引</span></span><br><span class="line"><span class="string">    :param a: 当data[A]== a时，说明该行样本时要保留的</span></span><br><span class="line"><span class="string">    :return: 新的数据集和标签集</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment">#返回的数据集</span></span><br><span class="line">    retDataArr = []</span><br><span class="line">    <span class="comment">#返回的标签集</span></span><br><span class="line">    retLabelArr = []</span><br><span class="line">    <span class="comment">#对当前数据的每一个样本进行遍历</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(trainDataArr)):</span><br><span class="line">        <span class="comment">#如果当前样本的特征为指定特征值a</span></span><br><span class="line">        <span class="keyword">if</span> trainDataArr[i][A] == a:</span><br><span class="line">            <span class="comment">#那么将该样本的第A个特征切割掉，放入返回的数据集中</span></span><br><span class="line">            retDataArr.append(trainDataArr[i][<span class="number">0</span>:A] + trainDataArr[i][A+<span class="number">1</span>:])</span><br><span class="line">            <span class="comment">#将该样本的标签放入返回标签集中</span></span><br><span class="line">            retLabelArr.append(trainLabelArr[i])</span><br><span class="line">    <span class="comment">#返回新的数据集和标签集</span></span><br><span class="line">    <span class="keyword">return</span> retDataArr, retLabelArr</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">createTree</span>(<span class="params">*dataSet</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    递归创建决策树</span></span><br><span class="line"><span class="string">    :param dataSet:(trainDataList， trainLabelList) &lt;&lt;-- 元祖形式</span></span><br><span class="line"><span class="string">    :return:新的子节点或该叶子节点的值</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment">#设置Epsilon，“5.3.1 ID3算法”第4步提到需要将信息增益与阈值Epsilon比较，若小于则</span></span><br><span class="line">    <span class="comment">#直接处理后返回T</span></span><br><span class="line">    <span class="comment">#该值的大小在设置上并未考虑太多，观察到信息增益前期在运行中为0.3左右，所以设置了0.1</span></span><br><span class="line">    Epsilon = <span class="number">0.1</span></span><br><span class="line">    <span class="comment">#从参数中获取trainDataList和trainLabelList</span></span><br><span class="line">    <span class="comment">#之所以使用元祖作为参数，是由于后续递归调用时直数据集需要对某个特征进行切割，在函数递归</span></span><br><span class="line">    <span class="comment">#调用上直接将切割函数的返回值放入递归调用中，而函数的返回值形式是元祖的，等看到这个函数</span></span><br><span class="line">    <span class="comment">#的底部就会明白了，这样子的用处就是写程序的时候简洁一点，方便一点</span></span><br><span class="line">    trainDataList = dataSet[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">    trainLabelList = dataSet[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line">    <span class="comment">#打印信息：开始一个子节点创建，打印当前特征向量数目及当前剩余样本数目</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;start a node&#x27;</span>, <span class="built_in">len</span>(trainDataList[<span class="number">0</span>]), <span class="built_in">len</span>(trainLabelList))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#将标签放入一个字典中，当前样本有多少类，在字典中就会有多少项</span></span><br><span class="line">    <span class="comment">#也相当于去重，多次出现的标签就留一次。举个例子，假如处理结束后字典的长度为1，那说明所有的样本</span></span><br><span class="line">    <span class="comment">#都是同一个标签，那就可以直接返回该标签了，不需要再生成子节点了。</span></span><br><span class="line">    classDict = &#123;i <span class="keyword">for</span> i <span class="keyword">in</span> trainLabelList&#125;</span><br><span class="line">    <span class="comment">#如果D中所有实例属于同一类Ck，则置T为单节点数，并将Ck作为该节点的类，返回T</span></span><br><span class="line">    <span class="comment">#即若所有样本的标签一致，也就不需要再分化，返回标记作为该节点的值，返回后这就是一个叶子节点</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(classDict) == <span class="number">1</span>:</span><br><span class="line">        <span class="comment">#因为所有样本都是一致的，在标签集中随便拿一个标签返回都行，这里用的第0个（因为你并不知道</span></span><br><span class="line">        <span class="comment">#当前标签集的长度是多少，但运行中所有标签只要有长度都会有第0位。</span></span><br><span class="line">        <span class="keyword">return</span> trainLabelList[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#如果A为空集，则置T为单节点数，并将D中实例数最大的类Ck作为该节点的类，返回T</span></span><br><span class="line">    <span class="comment">#即如果已经没有特征可以用来再分化了，就返回占大多数的类别</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(trainDataList[<span class="number">0</span>]) == <span class="number">0</span>:</span><br><span class="line">        <span class="comment">#返回当前标签集中占数目最大的标签</span></span><br><span class="line">        <span class="keyword">return</span> majorClass(trainLabelList)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#否则，按式5.10计算A中个特征值的信息增益，选择信息增益最大的特征Ag</span></span><br><span class="line">    Ag, EpsilonGet = calcBestFeature(trainDataList, trainLabelList)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#如果Ag的信息增益比小于阈值Epsilon，则置T为单节点树，并将D中实例数最大的类Ck</span></span><br><span class="line">    <span class="comment">#作为该节点的类，返回T</span></span><br><span class="line">    <span class="keyword">if</span> EpsilonGet &lt; Epsilon:</span><br><span class="line">        <span class="keyword">return</span> majorClass(trainLabelList)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#否则，对Ag的每一可能值ai，依Ag=ai将D分割为若干非空子集Di，将Di中实例数最大的</span></span><br><span class="line">    <span class="comment"># 类作为标记，构建子节点，由节点及其子节点构成树T，返回T</span></span><br><span class="line">    treeDict = &#123;Ag:&#123;&#125;&#125;</span><br><span class="line">    <span class="comment">#特征值为0时，进入0分支</span></span><br><span class="line">    <span class="comment">#getSubDataArr(trainDataList, trainLabelList, Ag, 0)：在当前数据集中切割当前feature，返回新的数据集和标签集</span></span><br><span class="line">    treeDict[Ag][<span class="number">0</span>] = createTree(getSubDataArr(trainDataList, trainLabelList, Ag, <span class="number">0</span>))</span><br><span class="line">    treeDict[Ag][<span class="number">1</span>] = createTree(getSubDataArr(trainDataList, trainLabelList, Ag, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> treeDict</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">testDataList, tree</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    预测标签</span></span><br><span class="line"><span class="string">    :param testDataList:样本</span></span><br><span class="line"><span class="string">    :param tree: 决策树</span></span><br><span class="line"><span class="string">    :return: 预测结果</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># treeDict = copy.deepcopy(tree)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#死循环，直到找到一个有效地分类</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="comment">#因为有时候当前字典只有一个节点</span></span><br><span class="line">        <span class="comment">#例如&#123;73: &#123;0: &#123;74:6&#125;&#125;&#125;看起来节点很多，但是对于字典的最顶层来说，只有73一个key，其余都是value</span></span><br><span class="line">        <span class="comment">#若还是采用for来读取的话不太合适，所以使用下行这种方式读取key和value</span></span><br><span class="line">        (key, value), = tree.items()</span><br><span class="line">        <span class="comment">#如果当前的value是字典，说明还需要遍历下去</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(tree[key]).__name__ == <span class="string">&#x27;dict&#x27;</span>:</span><br><span class="line">            <span class="comment">#获取目前所在节点的feature值，需要在样本中删除该feature</span></span><br><span class="line">            <span class="comment">#因为在创建树的过程中，feature的索引值永远是对于当时剩余的feature来设置的</span></span><br><span class="line">            <span class="comment">#所以需要不断地删除已经用掉的特征，保证索引相对位置的一致性</span></span><br><span class="line">            dataVal = testDataList[key]</span><br><span class="line">            <span class="keyword">del</span> testDataList[key]</span><br><span class="line">            <span class="comment">#将tree更新为其子节点的字典</span></span><br><span class="line">            tree = value[dataVal]</span><br><span class="line">            <span class="comment">#如果当前节点的子节点的值是int，就直接返回该int值</span></span><br><span class="line">            <span class="comment">#例如&#123;403: &#123;0: 7, 1: &#123;297:7&#125;&#125;，dataVal=0</span></span><br><span class="line">            <span class="comment">#此时上一行tree = value[dataVal]，将tree定位到了7，而7不再是一个字典了，</span></span><br><span class="line">            <span class="comment">#这里就可以直接返回7了，如果tree = value[1]，那就是一个新的子节点，需要继续遍历下去</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">type</span>(tree).__name__ == <span class="string">&#x27;int&#x27;</span>:</span><br><span class="line">                <span class="comment">#返回该节点值，也就是分类值</span></span><br><span class="line">                <span class="keyword">return</span> tree</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment">#如果当前value不是字典，那就返回分类值</span></span><br><span class="line">            <span class="keyword">return</span> value</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">model_test</span>(<span class="params">testDataList, testLabelList, tree</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    测试准确率</span></span><br><span class="line"><span class="string">    :param testDataList:待测试数据集</span></span><br><span class="line"><span class="string">    :param testLabelList: 待测试标签集</span></span><br><span class="line"><span class="string">    :param tree: 训练集生成的树</span></span><br><span class="line"><span class="string">    :return: 准确率</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment">#错误次数计数</span></span><br><span class="line">    errorCnt = <span class="number">0</span></span><br><span class="line">    <span class="comment">#遍历测试集中每一个测试样本</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(testDataList)):</span><br><span class="line">        <span class="comment">#判断预测与标签中结果是否一致</span></span><br><span class="line">        <span class="keyword">if</span> testLabelList[i] != predict(testDataList[i], tree):</span><br><span class="line">            errorCnt += <span class="number">1</span></span><br><span class="line">    <span class="comment">#返回准确率</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> - errorCnt / <span class="built_in">len</span>(testDataList)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment">#开始时间</span></span><br><span class="line">    start = time.time()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取训练集</span></span><br><span class="line">    trainDataList, trainLabelList = loadData(<span class="string">&#x27;../Mnist/mnist_train.csv&#x27;</span>)</span><br><span class="line">    <span class="comment"># 获取测试集</span></span><br><span class="line">    testDataList, testLabelList = loadData(<span class="string">&#x27;../Mnist/mnist_test.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#创建决策树</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;start create tree&#x27;</span>)</span><br><span class="line">    tree = createTree((trainDataList, trainLabelList))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;tree is:&#x27;</span>, tree)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#测试准确率</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;start test&#x27;</span>)</span><br><span class="line">    accur = model_test(testDataList, testLabelList, tree)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;the accur is:&#x27;</span>, accur)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#结束时间</span></span><br><span class="line">    end = time.time()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;time span:&#x27;</span>, end - start)</span><br></pre></td></tr></table></figure>

<h3 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a>C4.5</h3><figure class="highlight basic"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Function C4.<span class="number">5</span>(R:包含连续属性的无类别属性集合,C:类别属性,S:训练集)</span><br><span class="line">/*返回一棵决策树*/</span><br><span class="line">Begin</span><br><span class="line">   <span class="keyword">If</span> S为空,返回一个值为Failure的单个节点;</span><br><span class="line">   <span class="keyword">If</span> S是由相同类别属性值的记录组成,</span><br><span class="line">   <span class="keyword">If</span> R为空,则返回一个单节点,其值为在S的记录中找出的频率最高的类别属性值;</span><br><span class="line">   [注意未出现错误则意味着是不适合分类的记录]；</span><br><span class="line">  <span class="keyword">For</span> 所有的属性R(Ri) Do</span><br><span class="line">        <span class="keyword">If</span> 属性Ri为连续属性，则</span><br><span class="line">     Begin</span><br><span class="line">           将Ri的最小值赋给A1：</span><br><span class="line">        将Rm的最大值赋给Am；/*m值手工设置*/</span><br><span class="line">           <span class="keyword">For</span> j From <span class="number">2</span> <span class="keyword">To</span> m-<span class="number">1</span> Do Aj=A1+j*(A1Am)/m;</span><br><span class="line">           将Ri点的基于&#123;&lt; =Aj,&gt;Aj&#125;的最大信息增益属性(Ri,S)赋给A；</span><br><span class="line">     <span class="keyword">End</span>；</span><br><span class="line">  将R中属性之间具有最大信息增益的属性(D,S)赋给D;</span><br><span class="line">   将属性D的值赋给&#123;dj/j=<span class="number">1</span>,<span class="number">2</span>...m&#125;；</span><br><span class="line">  将分别由对应于D的值为dj的记录组成的S的子集赋给&#123;sj/j=<span class="number">1</span>,<span class="number">2</span>...m&#125;;</span><br><span class="line">   返回一棵树，其根标记为D;树枝标记为d1,d2...dm;</span><br><span class="line">   再分别构造以下树:</span><br><span class="line">   C4.<span class="number">5</span>(R-&#123;D&#125;,C,S1),C4.<span class="number">5</span>(R-&#123;D&#125;,C,S2)...C4.<span class="number">5</span>(R-&#123;D&#125;,C,Sm);</span><br><span class="line"><span class="keyword">End</span> C4.<span class="number">5</span></span><br></pre></td></tr></table></figure>

<h2 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h2><ul>
<li><a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/tree.html">scikit-learn 决策树指南</a></li>
<li>Decision trees and k Nearest Neighbors are covered practically in every ML book. We recommend “Pattern Recognition and Machine Learning” (C. Bishop) and “Machine Learning: A Probabilistic Perspective” (K. Murphy).</li>
<li>The book “Machine Learning in Action” (P. Harrington) will walk you through implementations of classic ML algorithms in pure Python.</li>
<li>Scipy 2017 <a target="_blank" rel="noopener" href="https://github.com/amueller/scipy-2017-sklearn">scikit-learn tutorial</a> by Alex Gramfort and Andreas Mueller.</li>
<li><a target="_blank" rel="noopener" href="https://github.com/rushter/MLAlgorithms">Implementations</a> of many ML algorithms. Good to search for decision trees and k-NN.</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li><a target="_blank" rel="noopener" href="https://mlcourse.ai/">mlcourse.ai</a> Author: <a target="_blank" rel="noopener" href="https://www.linkedin.com/in/festline/">Yury Kashnitsky</a>. Translated and edited by Christina Butsko, Gleb Filatov, and <a target="_blank" rel="noopener" href="https://www.linkedin.com/in/yuanyuanpao/">Yuanyuan Pao</a>.</li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/v_july_v/article/details/7577684">CSDN上的博客</a></li>
<li><a target="_blank" rel="noopener" href="http://scikit-learn.org/stable/documentation.html">Scikit-learn 文档</a></li>
<li><a target="_blank" rel="noopener" href="https://scikit-learn.org.cn/view/784.html">中文文档</a>和<a target="_blank" rel="noopener" href="http://scikit-learn.org.cn/view/89.html">中文教程</a></li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="reward-container">
  <div>谢谢支持！ Thanks for your support!</div>
  <button>
    赞赏
  </button>
  <div class="post-reward">
      <div>
        <img src="/images/donate_wechatpay.png" alt="Hetan 微信">
        <span>微信</span>
      </div>
      <div>
        <img src="/images/donate_alipay.png" alt="Hetan 支付宝">
        <span>支付宝</span>
      </div>

  </div>
</div>

          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>Hetan
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="http://hetan697.github.io/2022/07/11/%E5%86%B3%E7%AD%96%E6%A0%91/" title="决策树">http://hetan697.github.io/2022/07/11/决策树/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/07/08/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" rel="prev" title="目标检测">
                  <i class="fa fa-chevron-left"></i> 目标检测
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2022/07/12/scheme%E5%9F%BA%E7%A1%80/" rel="next" title="scheme基础">
                  scheme基础 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments utterances-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-face-kiss"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hetan</span>
</div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"hetan697/hetan697.github.io","issue_term":"title","theme":"boxy-light"}</script>
<script src="/js/third-party/comments/utterances.js"></script>

</body>
</html>
